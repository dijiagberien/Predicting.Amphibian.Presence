---
title: "Predicting Amphibian Presence"
author: "Adogbeji Agberien"
date: "06/05/2021"
output: 
  html_document: 
    keep_md: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background 

Ever so often, construction/development of infrastructure is preceeded by environmental impact assessments that need to identify characteristics such as amphibian breeding sites. Some challenges include 

- highly dispersed habitats in vast areas
- limited workers maybe due to budget
- limited time during survey due to urgency
- the availability of skilled workers to identify habitats 
- inconveninent weather conditions e.t.c.

The authors of the paper [here](https://doi.org/10.3390/ijgi8030123) propose using GIS and satellite imagery coupled with machine learning to identify locations where amphibians may be present. 

# The goal

The goal of this analysis is to predict the presence of amphibian species near the water reservoir based on features obtained from GIS systems and satellite images. Given that we are predicting the presence/absence, the task is a classification problem, and we will be testing several classification algorithms to determine which works best for this task. 

# Data source 

The data was downloaded from the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/Amphibians), and the associated scientific publication is linked [here](https://doi.org/10.3390/ijgi8030123). 

Let us begin by loading the some required packages

```{r warning = F, message = F}
# Install and load packages
required_packages <- c("tidyverse", "data.table", 
                       "Hmisc", 
                       "mlr3", "mlr3learners", "mlr3viz",
                       "rmarkdown", "knitr", "caret")

packageCheck <- lapply(required_packages, FUN = function(x) {
  if(!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
```

Let's take a peek at the data 

```{r echo = F}
amphibians_data <- fread("C:/Users/diji_/Desktop/Data Science/Projects/Amphibians/dataset.csv", skip = 1)
```

```{r}
# Get basic info about the data frame
cat("No of rows:", nrow(amphibians_data), "\nNo of columns:", ncol(amphibians_data))
```

There dataset is relatively small, containing `r nrow(amphibians_data)` rows and  `r ncol(amphibians_data)` variables. Some variables such as "ID" and "Motorway" are not required and will not be included in the analysis. The goal is to use environmental characteristics to predict the presence of amphibians. Let's take another look at the data to see the environmental variables, their summary statistics, and the amphibians whose presence we are trying to predict. 

```{r}
# See the first 3 rows of the data
head(amphibians_data, 3)
```

The meaning of the abbreviations for the predictor variables

1. SR: Surface of water reservoir (m^2^) - Numerical 
2. NR: Number of water reservoirs in habitat - Numerical 
3. TR: Type of water reservoir - Categorical
4. SUR1: Surrounding 1 (the dominant type of land cover surrounding the water reservoir) - Categorical
5. SUR2: Surrounding 2 (the second most dominant type of land cover surrounding the water reservoir) - Categorical
6. SUR3: Surrounding 3 (the third most dominant type of land cover surrounding the water reservoir) - Categorical
7. CR: Type of shore - Categorical
8. VR: Intensity of vegetation development - Categorical
9. MR: Maintenance status of the reservoir - Categorical
10. UR: Use of water reservoir - Categorical
11. FR: Presence of fishing - Categorical
12. BR: Minimum distance to building development - Numerical 
13. RR: Minimum distance to roads
14. Access from water table to land habitats - Categorical 

```{r}
# Get descriptive statistics about the data 
Hmisc::describe(amphibians_data)
```

What amphibian presence are we trying to predict? Let us begin with predicting green frog presence. 

```{r}
# Get names of amphibians
amphibian_names <- names(amphibians_data)[(ncol(amphibians_data) - 6) : ncol(amphibians_data)]
cat("The amphibians are", paste(1:7, ":", c(amphibian_names), collapse = "\n"), sep = "\n")
```

I'll be using the mlr3 package to implement the machine learning algorithms. Given that the task is classification, some likely contenders algorithms include logistic regression, support vector machines, naive bayes, decision trees, random forests, and neural networks. One way to go could be implementing all the listed ML methods and perform some k-fold cross validation to determine which algorithm works best for this task. The other is to ask ourselves the characteristics of our data and subsequently determine which would be sufficient for our task. 

I personally am a fan of the brute force method, but a major limitation is computational expense and time, which in all honesty makes the brute force approach illogical. I am still a fan of it though, because I get to evaluate the performance of each model, and it helps empirically improve my understanding of each method. 

Our data contains a mix of integers, categorical variables, and ordinal variables. According to this paper [here](https://doi.org/10.3390/ijgi8030123), methods like neural networks, and support vector machines were not utilized because of the preprocessing required when working with variables of different types (e.g. the need to onehotencode categorical variables, and scale numerical variables), They also mention that these steps introduce extra variance to the system and can decrease overall performance. Nonetheless, each method will still be tried in an attempt to evaluate the perfomance of each model, and further understand the nuances of each method. 

# Use of random forests to predict green frog presence. 

```{r}
# Data preprocessing 
green_frogs_data <- amphibians_data[, c(3:17)] %>% 
  .[, "BR" := factor(BR, order = T, levels = c(0, 1, 2, 5, 9, 10))] %>% 
  .[, "RR" := factor(RR, order = T, levels = c(0, 1, 2, 5, 9, 10))] %>% 
  .[, c("TR", "SUR1", "SUR2", "SUR3", "CR", "VR", "MR", "UR", "FR", "Green frogs") := 
      lapply(.SD, as.factor), 
    .SDcols = c("TR", "SUR1", "SUR2", "SUR3", "CR", "VR", "MR", "UR", "FR", "Green frogs")] %>% 
  setnames(old = "Green frogs", new = "green_frog_presence")
```

```{r}
# Create the task
task_green_frogs <- TaskClassif$new(id = "green_frogs", backend = green_frogs_data, target = "green_frog_presence")
print(task_green_frogs)
```

```{r}
# Split the data into training and test sets 
train_set <- sample(task_green_frogs$row_ids, 0.8*task_green_frogs$nrow)
test_set <- setdiff(task_green_frogs$row_ids, train_set)
```

```{r}
# Create a learner (Random Forests)
learner_rf_green_frogs <- lrn("classif.ranger", importance = "permutation")
print(learner_rf_green_frogs)
```

```{r}
# Train the random forest learner 
learner_rf_green_frogs$train(task_green_frogs, row_ids = train_set)
```

```{r}
# Let's get the feature importance 
set.seed(123)
learner_rf_green_frogs$importance() %>% 
  as.data.table(keep.rownames = T) %>% 
  setnames(new = c("Feature", "Importance")) %>% 
  ggplot(aes(x = reorder(Feature, Importance), y = Importance)) + 
  geom_col() +
  xlab("Feature") + ylab("Importance") +
  theme_classic()
```

Next question is can we improve our model performance by excluding some of the lesser important features? 

```{r}
# Make predictions on the test set using the created models 
green_frog_predictions_rf <- learner_rf_green_frogs$predict(task_green_frogs, row_ids = test_set)
```

```{r}
# Model evaluation 
green_frog_predictions_rf$confusion
```

# Next

  - Utilize other methods, e.g. to evaluate our model performance 

  - Evaluate other model performances on the data
  
  - Hyper-parameter tuning to improve model